{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqHoi4eTxiCF",
    "outputId": "790bbe08-db4f-41f3-b9c3-fde501a5693e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from mediapipe) (22.1.0)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: matplotlib in /home/g7/anaconda3/lib/python3.11/site-packages (from mediapipe) (3.7.1)\n",
      "Requirement already satisfied: numpy<2 in /home/g7/anaconda3/lib/python3.11/site-packages (from mediapipe) (1.24.3)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sentencepiece in /home/g7/anaconda3/lib/python3.11/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Collecting ml_dtypes>=0.4.0 (from jax->mediapipe)\n",
      "  Using cached ml_dtypes-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting opt_einsum (from jax->mediapipe)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scipy>=1.10 in /home/g7/anaconda3/lib/python3.11/site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/g7/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /home/g7/anaconda3/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /home/g7/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Downloading mediapipe-0.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m106.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:02\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m176.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:14\u001b[0m\n",
      "\u001b[?25hUsing cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading jax-0.4.36-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0mm\n",
      "\u001b[?25hDownloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl (100.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 MB\u001b[0m \u001b[31m203.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:16\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m419.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: flatbuffers, protobuf, opt_einsum, opencv-python, opencv-contrib-python, ml_dtypes, absl-py, sounddevice, jaxlib, jax, mediapipe\n",
      "Successfully installed absl-py-2.1.0 flatbuffers-24.3.25 jax-0.4.36 jaxlib-0.4.36 mediapipe-0.10.18 ml_dtypes-0.5.0 opencv-contrib-python-4.10.0.84 opencv-python-4.10.0.84 opt_einsum-3.4.0 protobuf-4.25.5 sounddevice-0.5.1\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g7/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/g7/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g7/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/g7/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/g7/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/g7/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/g7/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/g7/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0mm\n",
      "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Installing collected packages: namex, libclang, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, numpy, grpcio, google-pasta, gast, astunparse, tensorboard, rich, ml-dtypes, h5py, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.0\n",
      "    Uninstalling ml_dtypes-0.5.0:\n",
      "      Successfully uninstalled ml_dtypes-0.5.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.57.0 requires numpy<1.25,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "mediapipe 0.10.18 requires numpy<2, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 optree-0.13.1 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FmDQgHfuwJxj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 17:43:36.450576: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 17:43:36.450953: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-09 17:43:36.452831: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-09 17:43:36.457810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733759016.466092  111642 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733759016.468439  111642 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 17:43:36.476893: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZMoWUAuEE4du"
   },
   "outputs": [],
   "source": [
    "EXPECTED_KEYPOINTS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M6wLzCcxxny0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733759024.291916  111642 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1733759024.321330  111773 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.120), renderer: NVIDIA GeForce RTX 4080/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733759024.337783  111745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1733759024.360761  111765 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.3, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8lji50Zxx45k"
   },
   "outputs": [],
   "source": [
    "def get_bboxes(image): #using media pipe since im just testing\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    bounding_boxes = {\"left_hand\": None, \"right_hand\": None}\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            x_min, y_min = float('inf'), float('inf')\n",
    "            x_max, y_max = float('-inf'), float('-inf')\n",
    "\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x, y = landmark.x * image.shape[1], landmark.y * image.shape[0]\n",
    "                x_min, y_min = min(x_min, x), min(y_min, y)\n",
    "                x_max, y_max = max(x_max, x), max(y_max, y)\n",
    "\n",
    "            hand_label = \"left_hand\" if idx == 0 else \"right_hand\"\n",
    "            bounding_boxes[hand_label] = (int(x_min), int(y_min), int(x_max), int(y_max))\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F11ZXVWbwMnk"
   },
   "outputs": [],
   "source": [
    "def detect_keypoints(image, max_keypoints=200, target_length=None):\n",
    "    gray_hand = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_hand, None)\n",
    "\n",
    "    if descriptors is None:\n",
    "        descriptors = np.zeros((len(keypoints), 128))  # Ensure descriptors match number of keypoints\n",
    "\n",
    "    keypoints = sorted(keypoints, key=lambda kp: kp.response, reverse=True)\n",
    "\n",
    "    # Ensure exactly max_keypoints keypoints and descriptors\n",
    "    keypoints = keypoints[:max_keypoints]\n",
    "    descriptors = descriptors[:max_keypoints]  # Limit descriptors to match keypoints\n",
    "\n",
    "    keypoint_locations = [keypoint.pt for keypoint in keypoints]\n",
    "\n",
    "    # Pad with dummy keypoints and descriptors if necessary\n",
    "    if len(keypoints) < max_keypoints:\n",
    "        num_keypoints_to_add = max_keypoints - len(keypoints)\n",
    "\n",
    "        # Create dummy keypoints (pts = (0, 0)) and dummy descriptors (zeros)\n",
    "        dummy_keypoints = [(0.0, 0.0)] * num_keypoints_to_add  # List of (0, 0) tuples\n",
    "        dummy_descriptors = np.zeros((num_keypoints_to_add, 128))  # Empty descriptors for dummy keypoints\n",
    "\n",
    "        keypoint_locations.extend(dummy_keypoints)  # Add dummy keypoints\n",
    "        descriptors = np.vstack((descriptors, dummy_descriptors))  # Stack dummy descriptors\n",
    "\n",
    "    return keypoint_locations, descriptors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V3plfMjy5Nar"
   },
   "outputs": [],
   "source": [
    "def display_keypoints(image, bounding_boxes):\n",
    "\n",
    "    for label, coords in bounding_boxes.items():\n",
    "        if coords:\n",
    "            x_min, y_min, x_max, y_max = coords\n",
    "            cropped_hand = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "            keypoints = detect_keypoints(cropped_hand)\n",
    "\n",
    "            for keypoint in keypoints:\n",
    "                x, y = keypoint\n",
    "                cv2.circle(cropped_hand, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "    cv2_imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4ugmULfx9X8"
   },
   "outputs": [],
   "source": [
    "input_image = cv2.imread('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "yZ7E2DRKyjys",
    "outputId": "32b49c63-1b28-4e36-9052-04c8897fdcb8"
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-c121a2d565f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbounding_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-158-7635ce9c5e18>\u001b[0m in \u001b[0;36mget_bboxes\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#using media pipe since im just testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimage_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbounding_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"left_hand\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right_hand\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "bounding_boxes = get_bboxes(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "ZRRoVtmv0R6S",
    "outputId": "48eaca49-570d-4b52-ed19-d19b6c56b144"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bounding_boxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-9529e45ec434>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounding_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bounding_boxes' is not defined"
     ]
    }
   ],
   "source": [
    "display_keypoints(input_image, bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9dsMaXnz194p"
   },
   "outputs": [],
   "source": [
    "def generate_dummy_keypoints_and_descriptors(max_keypoints=200):\n",
    "    # Create dummy keypoints as (0.0, 0.0) pairs\n",
    "    dummy_keypoints = [(0.0, 0.0)] * max_keypoints\n",
    "\n",
    "    # Create dummy descriptors as arrays of zeros (shape: (max_keypoints, 128))\n",
    "    dummy_descriptors = np.zeros((max_keypoints, 128))\n",
    "\n",
    "    return dummy_keypoints, dummy_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nxFMvF353_Tm"
   },
   "outputs": [],
   "source": [
    "MAX_HANDS =2\n",
    "def load_data_from_directory(root_directory, total_images, data_fraction=1.0):\n",
    "    \"\"\"\n",
    "    Load data from the given directory, with an option to limit the amount of data by a fraction.\n",
    "\n",
    "    Parameters:\n",
    "    - root_directory: Path to the root directory containing image files.\n",
    "    - total_images: Total number of images available in the directory.\n",
    "    - data_fraction: A decimal value representing the fraction of the data to load (e.g., 0.8 for 80%).\n",
    "\n",
    "    Returns:\n",
    "    - X: List of keypoints for each image.\n",
    "    - y: List of corresponding labels for each image.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Calculate the number of images to process based on the given fraction\n",
    "    max_images_to_process = int(total_images * data_fraction)\n",
    "    processed_images = 0\n",
    "\n",
    "    with tqdm(total=max_images_to_process, desc=\"Processing Images\", unit=\"image\") as pbar:\n",
    "        for label in os.listdir(root_directory):\n",
    "            label_path = os.path.join(root_directory, label)\n",
    "\n",
    "            if os.path.isdir(label_path):\n",
    "                for subdir, _, files in os.walk(label_path):\n",
    "                    for file in files:\n",
    "                        if file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                            if processed_images >= max_images_to_process:\n",
    "                              return np.array(X), np.array(y)\n",
    "                            try:\n",
    "                              image_path = os.path.join(subdir, file)\n",
    "                              image = cv2.imread(image_path)\n",
    "                              bounding_boxes = get_bboxes(image)\n",
    "                              all_keypoints = []\n",
    "                              hands_detected = False\n",
    "                              for hand_label, bounding_box in bounding_boxes.items():\n",
    "                                  if bounding_box:  # If bounding box exists\n",
    "                                      hands_detected = True\n",
    "                                      x_min, y_min, x_max, y_max = bounding_box\n",
    "                                      cropped_hand = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "                                      keypoints, descriptors = detect_keypoints(cropped_hand)\n",
    "                                      all_keypoints = keypoints\n",
    "                              if hands_detected:\n",
    "                                  num_keypoints = len(all_keypoints)\n",
    "                                  target_length = EXPECTED_KEYPOINTS * MAX_HANDS\n",
    "                                  if num_keypoints == 0:\n",
    "                                      pbar.update(1)\n",
    "                                      continue\n",
    "                                  elif num_keypoints < target_length:\n",
    "                                      dummy_keypoints, dummy_descriptors = generate_dummy_keypoints_and_descriptors(target_length - num_keypoints)\n",
    "                                      all_keypoints.extend(dummy_keypoints)\n",
    "                                      descriptors = np.vstack([descriptors, dummy_descriptors])\n",
    "\n",
    "\n",
    "                                  elif num_keypoints > target_length:\n",
    "                                    print(\"MoORE\")\n",
    "                                      # all_keypoints = all_keypoints[:target_length]\n",
    "                                      # for descriptor in descriptors:\n",
    "                                      #     descriptor = descriptor[:target_length]\n",
    "                                      # keypoint_descriptors = np.concatenate([np.array(all_keypoints).flatten(), descriptors.flatten()])\n",
    "                                  keypoint_descriptors = np.concatenate([np.array(all_keypoints).flatten(), descriptors.flatten()])\n",
    "                                  X.append(keypoint_descriptors)\n",
    "                                  y.append(label)\n",
    "                            except:\n",
    "                              pass\n",
    "\n",
    "                            processed_images += 1\n",
    "                            pbar.update(1)\n",
    "\n",
    "                    if processed_images >= max_images_to_process:\n",
    "                        break  # Break the outer loop if we have processed enough images\n",
    "\n",
    "            if processed_images >= max_images_to_process:\n",
    "                break  # Break the main loop if we have processed enough images\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F0qmOXxS0Zpt"
   },
   "outputs": [],
   "source": [
    " # Normalize the landmarks to [0, 1]\n",
    "def preprocess_landmarks(keypoints, image_width, image_height):\n",
    "    processed_landmarks = []\n",
    "    for i in keypoints:\n",
    "        x, y = i\n",
    "        processed_landmarks.append(x / image_width)\n",
    "        processed_landmarks.append(y / image_height)\n",
    "\n",
    "    return np.array(processed_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "204xKhDVNIQI"
   },
   "outputs": [],
   "source": [
    "def build_fc_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    model.add(layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dense(len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # Learning rate adjustment\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sIQ6-XFg3RVx"
   },
   "outputs": [],
   "source": [
    "def train_model(X, y, epochs=10, batch_size=32):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    scaler = StandardScaler()\n",
    "    X_encoded = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "    model = build_fc_model(input_shape=(X_train.shape[1], ))\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test)\n",
    "    )\n",
    "\n",
    "    return model, scaler, label_encoder, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5IKy7xfl3dYm"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The way this function works is as follows\n",
    "\n",
    "1- loop over a directory\n",
    "2- for each item in the directory check if its an image\n",
    "3- if its an image take the first item after splitting by underscore\n",
    "4- That item is the label, for example hello_4.jpg will be labeled as hello, so make sure the images should be stored that way\n",
    "5- Bounding boxes are extracted\n",
    "6- For each bounding box we get the key points\n",
    "7- If exists we then normaize it using the preprocess landmarks function\n",
    "8- We add it them to the x and y arrays that will be used to train the model\n",
    "\"\"\"\n",
    "def predict_sign_language(model, label_encoder, scaler, image):\n",
    "    bounding_boxes = get_bboxes(image)\n",
    "    all_keypoints = []  # This will hold the keypoints for all detected hands\n",
    "    hands_detected = False\n",
    "    for _, bounding_box in bounding_boxes.items():\n",
    "        if bounding_box:\n",
    "            hands_detected = True\n",
    "            x_min, y_min, x_max, y_max = bounding_box\n",
    "            cropped_hand = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            keypoints = detect_keypoints(cropped_hand)\n",
    "\n",
    "            if keypoints:\n",
    "                processed_keypoints = preprocess_landmarks(keypoints, image.shape[1], image.shape[0])\n",
    "                all_keypoints.extend(processed_keypoints)\n",
    "    if hands_detected:\n",
    "        num_keypoints = len(all_keypoints)\n",
    "        if num_keypoints == 0:\n",
    "          return None\n",
    "        elif num_keypoints < EXPECTED_KEYPOINTS * MAX_HANDS:\n",
    "            all_keypoints.extend([0] * (EXPECTED_KEYPOINTS * MAX_HANDS - num_keypoints))\n",
    "        elif num_keypoints > EXPECTED_KEYPOINTS * MAX_HANDS:\n",
    "            all_keypoints = all_keypoints[:EXPECTED_KEYPOINTS * MAX_HANDS]\n",
    "        processed_keypoints = np.array(all_keypoints).reshape(1, -1, 1)\n",
    "        processed_keypoints = scaler.transform(processed_keypoints)\n",
    "        prediction = model.predict(processed_keypoints)\n",
    "        predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "\n",
    "        return predicted_label[0]\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "C93-CXyvhsjZ"
   },
   "outputs": [],
   "source": [
    "def save_data_to_file(X, y, file_name='data.npz'):\n",
    "    np.savez(file_name, X=X, y=y)\n",
    "    print(f\"Data saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CEXPYcElhs35"
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(file_name='data.npz'):\n",
    "    data = np.load(file_name)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    print(f\"Data loaded from {file_name}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aepq6h2Be3Hx",
    "outputId": "c71dea85-c420-474e-af6d-8c1c47eede7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JRi6DxGQkKG6"
   },
   "outputs": [],
   "source": [
    "images_dir = '/content/drive/MyDrive/compFrames/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-e9yxmQt3d_a",
    "outputId": "690b86be-3f74-4464-ab44-aef4ede6b24a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 16500/16500 [2:32:27<00:00,  1.80image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data_sift.npz\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data_from_directory(images_dir,165000,0.1)\n",
    "save_data_to_file(X, y, file_name='data_sift.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "FJIJdpsj11Xm",
    "outputId": "1f62d46e-69ff-4d90-879d-fc14b261158d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from data_sift.npz\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data_from_file('data_sift.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYDhAc3B2oh1",
    "outputId": "28c7d7e8-25e4-4bec-adad-c9277e663694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /content/drive/MyDrive/data_sift.npz\n"
     ]
    }
   ],
   "source": [
    "save_data_to_file(X, y, file_name='/content/drive/MyDrive/data_sift.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCRT7Jdiz6iV",
    "outputId": "429ea995-8d77-43ab-e202-c7a9e383e141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['above' 'accomplish' 'adopt' 'advantage' 'alphabet' 'anniversary'\n",
      " 'another' 'appropriate' 'arrogant' 'artist' 'background' 'bee' 'behavior'\n",
      " 'bell' 'birth' 'bless' 'blood' 'boots' 'brave' 'breathe' 'bright' 'bull'\n",
      " 'butterfly' 'card' 'category' 'catholic' 'cent' 'chemistry' 'cherry'\n",
      " 'china' 'chop' 'christian' 'cigarette' 'clever' 'clown' 'coach' 'coat'\n",
      " 'cochlear implant' 'coconut' 'common' 'commute' 'control' 'count'\n",
      " 'culture' 'daily' 'defend' 'degree' 'demonstrate' 'department' 'die'\n",
      " 'dig' 'dinner' 'dinosaur' 'diploma' 'director' 'disconnect' 'discover'\n",
      " \"don't want\" 'drum' 'each' 'educate' 'electrician' 'empty' 'england'\n",
      " 'establish' 'excuse' 'eyeglasses' 'faculty' 'fall in love' 'famous'\n",
      " 'farm' 'fence' 'fix' 'flexible' 'flirt' 'flood' 'fork' 'four' 'fun'\n",
      " 'funeral' 'furniture' 'gallaudet' 'general' 'ghost' 'give up' 'glass'\n",
      " 'gorilla' 'gray' 'guide' 'habit' 'health' 'hearing aid' 'heaven' 'heavy'\n",
      " 'helmet' 'hide' 'high school' 'hockey' 'hold' 'honest' 'hug' 'hunt'\n",
      " 'image' 'influence' 'interesting' 'interpreter' 'iran' 'italy' 'jewish'\n",
      " 'judge' 'key' 'label' 'leaf' 'left' 'lend' 'less' 'linguistics' 'lonely'\n",
      " 'long' 'magic' 'mainstream' 'major' 'manager' 'maximum' 'me' 'mexico'\n",
      " 'middle' 'mind' 'misunderstand' 'monster' 'multiply' 'myself' 'nothing'\n",
      " 'october' 'odd' 'offer' 'on' 'only' 'opposite' 'out' 'overlook' 'path'\n",
      " 'percent' 'physics' 'piano' 'pick' 'pie' 'plate' 'pocket' 'popcorn'\n",
      " 'positive' 'pregnant' 'prevent' 'proof' 'pumpkin' 'purpose' 'race'\n",
      " 'realize' 'recognize' 'refuse' 'relationship' 'repeat' 'replace' 'report'\n",
      " 'represent' 'request' 'responsibility' 'result' 'reveal' 'ring' 'rob'\n",
      " 'rock' 'role' 'rope' 'rush' 'salute' 'satisfy' 'search' 'selfish'\n",
      " 'sensitive' 'serve' 'seven' 'shampoo' 'she' 'support' 'suppose' 'sure'\n",
      " 'surprise' 'sweater' 'swim' 'symbol' 'taste' 'team' 'telephone' 'tennis'\n",
      " 'their' 'then' 'thermometer' 'thing' 'third' 'thousand' 'three' 'tie'\n",
      " 'topic' 'touch' 'tournament' 'try' 'two' 'type' 'umbrella' 'vegetable'\n",
      " 'vocabulary' 'waste' 'we' 'wear' 'weekly' 'welcome' 'winter' 'without'\n",
      " 'wolf' 'worm' 'wristwatch' 'yourself']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwSD_6IfBRJr",
    "outputId": "19849ce3-c79c-40af-d895-bc8ddc39cc5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1733759079.557999  111642 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 151ms/step - accuracy: 0.0089 - loss: 7.6525 - val_accuracy: 0.0131 - val_loss: 6.7722\n",
      "Epoch 2/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.0169 - loss: 6.8591 - val_accuracy: 0.0197 - val_loss: 6.7964\n",
      "Epoch 3/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 149ms/step - accuracy: 0.0336 - loss: 6.6946 - val_accuracy: 0.0306 - val_loss: 6.7763\n",
      "Epoch 4/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.0514 - loss: 6.5485 - val_accuracy: 0.0328 - val_loss: 6.9329\n",
      "Epoch 5/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 149ms/step - accuracy: 0.0682 - loss: 6.4727 - val_accuracy: 0.0284 - val_loss: 6.8784\n",
      "Epoch 6/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 149ms/step - accuracy: 0.0989 - loss: 6.3410 - val_accuracy: 0.0438 - val_loss: 6.9273\n",
      "Epoch 7/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.1192 - loss: 6.2594 - val_accuracy: 0.0361 - val_loss: 7.2161\n",
      "Epoch 8/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.1309 - loss: 6.2944 - val_accuracy: 0.0470 - val_loss: 7.2119\n",
      "Epoch 9/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.1555 - loss: 6.2090 - val_accuracy: 0.0383 - val_loss: 7.3726\n",
      "Epoch 10/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.1742 - loss: 6.2444 - val_accuracy: 0.0416 - val_loss: 7.3924\n",
      "Epoch 11/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.1953 - loss: 6.1627 - val_accuracy: 0.0372 - val_loss: 7.3920\n",
      "Epoch 12/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.2073 - loss: 6.0867 - val_accuracy: 0.0503 - val_loss: 7.5066\n",
      "Epoch 13/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.2270 - loss: 6.0655 - val_accuracy: 0.0449 - val_loss: 7.5116\n",
      "Epoch 14/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.2308 - loss: 6.0114 - val_accuracy: 0.0438 - val_loss: 7.6041\n",
      "Epoch 15/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.2492 - loss: 5.9454 - val_accuracy: 0.0492 - val_loss: 7.5337\n",
      "Epoch 16/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.2537 - loss: 5.9582 - val_accuracy: 0.0624 - val_loss: 7.6554\n",
      "Epoch 17/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.2730 - loss: 5.9255 - val_accuracy: 0.0481 - val_loss: 7.6133\n",
      "Epoch 18/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.2771 - loss: 5.8695 - val_accuracy: 0.0536 - val_loss: 7.5992\n",
      "Epoch 19/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 149ms/step - accuracy: 0.3086 - loss: 5.7526 - val_accuracy: 0.0722 - val_loss: 7.6145\n",
      "Epoch 20/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.3016 - loss: 5.7638 - val_accuracy: 0.0558 - val_loss: 7.5905\n",
      "Epoch 21/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.3206 - loss: 5.5791 - val_accuracy: 0.0580 - val_loss: 7.5744\n",
      "Epoch 22/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 152ms/step - accuracy: 0.3251 - loss: 5.6288 - val_accuracy: 0.0536 - val_loss: 7.6227\n",
      "Epoch 23/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.3279 - loss: 5.6003 - val_accuracy: 0.0635 - val_loss: 7.6246\n",
      "Epoch 24/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.3589 - loss: 5.4137 - val_accuracy: 0.0525 - val_loss: 7.6893\n",
      "Epoch 25/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 152ms/step - accuracy: 0.3528 - loss: 5.4358 - val_accuracy: 0.0700 - val_loss: 7.6503\n",
      "Epoch 26/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 152ms/step - accuracy: 0.3721 - loss: 5.3378 - val_accuracy: 0.0602 - val_loss: 7.7815\n",
      "Epoch 27/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 152ms/step - accuracy: 0.3713 - loss: 5.3808 - val_accuracy: 0.0722 - val_loss: 7.6180\n",
      "Epoch 28/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 153ms/step - accuracy: 0.3793 - loss: 5.2636 - val_accuracy: 0.0635 - val_loss: 7.6981\n",
      "Epoch 29/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.3750 - loss: 5.2688 - val_accuracy: 0.0503 - val_loss: 7.6755\n",
      "Epoch 30/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.3738 - loss: 5.2744 - val_accuracy: 0.0547 - val_loss: 7.7109\n",
      "Epoch 31/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.3925 - loss: 5.1561 - val_accuracy: 0.0547 - val_loss: 7.6028\n",
      "Epoch 32/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4025 - loss: 5.0615 - val_accuracy: 0.0624 - val_loss: 7.5631\n",
      "Epoch 33/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.4191 - loss: 5.0152 - val_accuracy: 0.0656 - val_loss: 7.5840\n",
      "Epoch 34/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.4108 - loss: 5.0162 - val_accuracy: 0.0591 - val_loss: 7.6167\n",
      "Epoch 35/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.4225 - loss: 4.9224 - val_accuracy: 0.0569 - val_loss: 7.7413\n",
      "Epoch 36/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4155 - loss: 4.9113 - val_accuracy: 0.0700 - val_loss: 7.5300\n",
      "Epoch 37/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 151ms/step - accuracy: 0.4252 - loss: 4.8453 - val_accuracy: 0.0689 - val_loss: 7.5737\n",
      "Epoch 38/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4145 - loss: 4.9248 - val_accuracy: 0.0624 - val_loss: 7.7614\n",
      "Epoch 39/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4289 - loss: 4.8191 - val_accuracy: 0.0744 - val_loss: 7.5733\n",
      "Epoch 40/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 148ms/step - accuracy: 0.4337 - loss: 4.7242 - val_accuracy: 0.0711 - val_loss: 7.6521\n",
      "Epoch 41/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 148ms/step - accuracy: 0.4401 - loss: 4.7502 - val_accuracy: 0.0700 - val_loss: 7.5534\n",
      "Epoch 42/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4376 - loss: 4.7207 - val_accuracy: 0.0656 - val_loss: 7.6140\n",
      "Epoch 43/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4352 - loss: 4.6997 - val_accuracy: 0.0678 - val_loss: 7.6710\n",
      "Epoch 44/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4249 - loss: 4.7257 - val_accuracy: 0.0689 - val_loss: 7.5693\n",
      "Epoch 45/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4488 - loss: 4.5600 - val_accuracy: 0.0711 - val_loss: 7.6514\n",
      "Epoch 46/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4675 - loss: 4.5074 - val_accuracy: 0.0613 - val_loss: 7.6313\n",
      "Epoch 47/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4624 - loss: 4.4843 - val_accuracy: 0.0689 - val_loss: 7.4958\n",
      "Epoch 48/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4554 - loss: 4.5132 - val_accuracy: 0.0667 - val_loss: 7.6478\n",
      "Epoch 49/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4761 - loss: 4.4337 - val_accuracy: 0.0591 - val_loss: 7.6786\n",
      "Epoch 50/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 150ms/step - accuracy: 0.4551 - loss: 4.5108 - val_accuracy: 0.0689 - val_loss: 7.6435\n"
     ]
    }
   ],
   "source": [
    "model,scaler,label_encoder,history = train_model(X, y,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSanNjQp37ea"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('test_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skioJ91R39tA"
   },
   "outputs": [],
   "source": [
    "###seems to need reset after running tensorflow probably something with the cache\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wDW7jF73J9w",
    "outputId": "daee0b22-e37c-4b91-bcc7-3c6c6eec979a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left_hand': (488, 447, 703, 719), 'right_hand': None}\n"
     ]
    }
   ],
   "source": [
    "bounding_boxes = get_bboxes(image)\n",
    "print(bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0CSzhvvvuci",
    "outputId": "39f2042e-2295-42ed-a2d0-c5378b7b9756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_sign = predict_sign_language(model,label_encoder, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNjrJbGB36nN",
    "outputId": "49215917-8af6-417e-c288-ff00c9d3113e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sign language gesture: thanks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted sign language gesture: {predicted_sign}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrXH8-THvjRs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
